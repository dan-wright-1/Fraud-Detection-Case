leo_split %>% testing()
leo <- titanic %>% mutate(across(c(survived, pclass, had_cabin, sex), ~ as.factor(.x)))
leo_split <-leo %>%
initial_split(prop = .8,
strata = survived)
leo_split %>% training()
leo_split %>% testing()
leo_trainig <- leo_split %>% training()
leo_testing <-  %>% testing()
# Plan the model setup, including the engine and mode
leo_model <- logisitic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
leo_model
# Now fit a model, look at output with tidy()
leo_model %>%
fit(survived ~ .,
data = leo_split)
leo_testing <- leo_split %>% testing()
# Plan the model setup, including the engine and mode
leo_model <- logisitic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
leo_model
library(tidyverse)
library(tidymodels)
# Okay let's work through the (cleaned) titanic data from last week:
titanic <- read_csv('https://www.dropbox.com/s/92funarubgk5rzh/titanic_clean.csv?dl=1')
# First let's make sure factors are factors
leo <- titanic %>% mutate(across(c(survived, pclass, had_cabin, sex), ~ as.factor(.x)))
set.seed(42)
# Now let's do a train/test split
leo_split <-leo %>%
initial_split(prop = .8,
strata = survived)
leo_trainig <- leo_split %>% training()
leo_testing <- leo_split %>% testing()
# Plan the model setup, including the engine and mode
leo_model <- logisitic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
# Plan the model setup, including the engine and mode
leo_model <- logistic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
leo_model
# Now fit a model, look at output with tidy()
leo_model %>%
fit(survived ~ .,
data = leo_split)
leo_model
# Now fit a model, look at output with tidy()
leo_fit <- leo_model %>%
fit(survived ~ .,
data = leo_split)
leo_fit %>% tidy()
# Now fit a model, look at output with tidy()
leo_fit <- leo_model %>%
fit(survived ~ .,
data = leo_split)
leo_fit %>% tidy()
library(tidyverse)
library(tidymodels)
# Okay let's work through the (cleaned) titanic data from last week:
titanic <- read_csv('https://www.dropbox.com/s/92funarubgk5rzh/titanic_clean.csv?dl=1')
# First let's make sure factors are factors
leo <- titanic %>% mutate(across(c(survived, pclass, had_cabin, sex), ~ as.factor(.x)))
set.seed(42)
# Now let's do a train/test split
leo_split <-leo %>%
initial_split(prop = .8,
strata = survived)
leo_trainig <- leo_split %>% training()
leo_testing <- leo_split %>% testing()
# Plan the model setup, including the engine and mode
leo_model <- logistic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
# relevant model types: logistic_reg(), linear_reg(), decision_tree(), rand_forest(), boost_tree()
# show_engines('logistic_reg')
# Now fit a model, look at output with tidy()
leo_fit <- leo_model %>%
fit(survived ~ .,
data = leo_split)
library(tidyverse)
library(tidymodels)
# Okay let's work through the (cleaned) titanic data from last week:
titanic <- read_csv('https://www.dropbox.com/s/92funarubgk5rzh/titanic_clean.csv?dl=1')
# First let's make sure factors are factors
leo <- titanic %>% mutate(across(c(survived, pclass, had_cabin, sex), ~ as.factor(.x)))
set.seed(42)
# Now let's do a train/test split
leo_split <-leo %>%
initial_split(prop = .8,
strata = survived)
leo_trainig <- leo_split %>% training()
leo_testing <- leo_split %>% testing()
# Plan the model setup, including the engine and mode
leo_model <- logistic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
# relevant model types: logistic_reg(), linear_reg(), decision_tree(), rand_forest(), boost_tree()
# show_engines('logistic_reg')
# Now fit a model, look at output with tidy()
leo_fit <- leo_model %>%
fit(survived ~ .,
data = leo_split)
library(tidyverse)
library(tidymodels)
library(lubridate)
# Warm Up Exercise --------------------------------------------------------
cr_data <- read_csv('https://www.dropbox.com/scl/fi/vykejw5ud9ejjvcc442gd/credit_small.csv?rlkey=zuyurxikxickgdjchh6681j91&dl=1')
cr_data <- cr_data %>%
mutate(status = as.factor(status))
cr_data %>% glimpse()
# Missingness:
cr_data %>%
summarise(across(everything(), ~sum(is.na(.)))) %>%
glimpse()
# Numeric distributions:
cr_data %>%
select(where(is.numeric)) %>%
pivot_longer(everything()) %>%
ggplot(aes(x = value, fill = name)) +
geom_histogram(alpha = .4) +
facet_wrap(~name, scales = 'free') +
labs(title = 'Distributions of Numeric Features',
fill = 'Feature')
# Model setup:
set.seed(42)
cr_split <- initial_split(cr_data, strata = status)
cr_training <- cr_split %>% training()
cr_testing <- cr_split %>% testing()
# Let's create a recipe that:
#    - imputes missing numeric values
#    - log transforms assets, debt, income, price, expenses
#    - normalizes all numeric predictors
#    - dummy codes all categories
cr_rec <- recipe(status ~ .,
data = cr_training) %>%
step_impute_median(all_numeric_predictors()) %>%
step_log(assets, debt, income, price, expenses, offset = 1) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
cars <- read_csv('https://www.dropbox.com/scl/fi/xavej23qpauvx3xfdq7zh/car_sales.csv?rlkey=4mfp6tpia0uqkcoiqf9jleau3&dl=1')
library(tidyverse)
library(tidymodels)
library(lubridate)
# With a messier dataset: -------------------------------------------------
cars <- read_csv('https://www.dropbox.com/scl/fi/xavej23qpauvx3xfdq7zh/car_sales.csv?rlkey=4mfp6tpia0uqkcoiqf9jleau3&dl=1')
cars %>% glimpse
# missingness counts:
cars %>% summarize(across(everything(), ~sum(is.na(.)))) %>% glimpse
# There are lots of makes and even more models
cars %>% count(make)
cars %>% count(model) %>% arrange(-n)
# Quick look at the distributions of numerics:
cars %>%
select(where(is.numeric)) %>%
pivot_longer(everything()) %>%
ggplot(aes(x = value, fill = name)) +
geom_histogram(alpha = .4) +
facet_wrap(~name, scales = 'free') +
labs(title = 'Distributions of Numeric Features',
fill = 'Feature')
set.seed(42)
cars_split <- initial_split(cars, strata = sellingprice_log)
cars_training <- cars_split %>% training()
cars_testing <- cars_split %>% testing()
# Create a recipe that handles:
#     - median imputation for all numeric features
#     - YeoJohnson transformation of all numeric features
#     - missingness in make/model
#     - long-tail (i.e., uncommon) values in make/model
#     - Normalize all numeric features
#     - dummy coding for all categories
#
# New steps to try: step_YeoJohnson(), step_unknown(), step_other()
cars_rec <- recipe(sellingprice_log ~ .,
data = cars_training) %>%
step_impute_median(all_numeric_predictors()) %>%
step_YeoJohnson(all_numeric_predictors()) %>%
step_unknown(c(make, model), new_level = 'this_was_missing') %>%
step_other(c(make, model), threshold = .05) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
# Look at the effect of step_unknown() and step_other()
cars_rec %>% prep() %>% bake(new_data = cars_training) %>%
glimpse()
cars_rec %>% prep() %>% bake(new_data = cars_training) %>%
count(model)
library(tidyverse)
library(tidymodels)
library(janitor)
install.packages("skimr")
library(skimr)
install.packages("themis")
library(themis)
library(forcats)
tidymodels_prefer()
install.packages("ranger")
# Set your own script to work in the right local directory:
setwd('C:\\Users\\dwrig\\Documents\\GitHub\\is-555-10-modeling-pipeline-2-dan-wright-1')
set.seed(42)
### 1. Data Cleaning and Preparation
marketing_raw <- read_csv('https://www.dropbox.com/scl/fi/aydmw9exui5hmns442dnt/14_train.csv?rlkey=gue9zu9q1upxliutws82svmn9&dl=1')
marketing_raw <- marketing_raw %>% clean_names()
marketing <-marketing_raw %>%
mutate(response = as.factor(response)) %>%
select(everything(), -id)
marketing %>% glimpse()
marketing %>% skim()
missing_summary <- marketing %>%
summarise_all(~ sum(is.na(.))) %>%
pivot_longer(cols = everything(), names_to = "column", values_to = "na_count")
print(missing_summary)
marketing %>%
select_if(is.numeric) %>%
summary()
marketing %>%
select_if(is.numeric) %>%
pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
ggplot(aes(x = variable, y = value)) +
geom_boxplot() +
facet_wrap(~ variable, scales = "free") +
theme_minimal() +
labs(title = "Boxplots of Numeric Variables", x = "Variable", y = "Value")
ggplot(marketing, aes(x = year_birth)) +
geom_histogram(binwidth = 5, fill = "grey", color = "black") +
labs(title = "Distribution of Year of Birth", x = "Year of Birth", y = "Count")
ggplot(marketing, aes(x = income)) +
geom_histogram(bins = 30, fill = "grey", color = "black") +
labs(title = "Distribution of Income", x = "Income", y = "Count") +
scale_x_continuous(labels = scales::comma)
marketing %>%
select(mnt_wines, mnt_meat_products, mnt_fish_products, mnt_sweet_products, mnt_gold_prods) %>%
pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
ggplot(aes(x = variable, y = value)) +
geom_boxplot() +
facet_wrap(~ variable, scales = "free") +
labs(title = "Boxplots of Spending Variables", x = "Variable", y = "Value") +
theme_minimal()
marketing %>%
select(mnt_wines, mnt_meat_products, mnt_fish_products, mnt_sweet_products, mnt_gold_prods) %>%
pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
ggplot(aes(x = value)) +
geom_histogram(bins = 30, fill = "grey", color = "black") +
facet_wrap(~ variable, scales = "free_x") +
labs(title = "Histograms of Spending Categories", x = "Amount", y = "Count") +
theme_minimal() +
scale_x_continuous(labels = comma)
#removing because they are constants
glimpse(marketing)
marketing <- marketing %>%
select(-z_cost_contact, -z_revenue)
#handling outliers for income and birth year
# For income: cap values above the 99th percentile
income_99 <- quantile(marketing$income, 0.99, na.rm = TRUE)
marketing <- marketing %>%
mutate(income = if_else(income > income_99, income_99, income))
# For year_birth: floor values below the 1st percentile
year_birth_01 <- quantile(marketing$year_birth, 0.01, na.rm = TRUE)
marketing <- marketing %>%
mutate(year_birth = if_else(year_birth < year_birth_01, year_birth_01, year_birth))
#impute missing income data
income_median <- median(marketing$income, na.rm = TRUE)
marketing <- marketing %>%
mutate(income = if_else(is.na(income), income_median, income))
data_clean <- marketing
data_clean %>% skim()
marketing %>%
summarise(across(everything(), ~n_distinct(.)))
threshold <- nrow(data_clean) * 0.03
# Recode marital_status: levels with counts below the threshold become "other"
data_clean <- data_clean %>%
mutate(marital_status = fct_lump_min(marital_status, min = threshold, other_level = "other"))
### 2. Train/Test Split
data_split <- initial_split(data_clean, prop = 0.75, strata = response)
data_training <- data_split %>% training()
data_testing <- data_split %>% testing()
data_training %>%
count(education) %>%
arrange(desc(n))
data_training %>%
count(marital_status) %>%
arrange(desc(n))
### 3. Feature Engineering Recipes
recipe_1 <- recipe(response ~ ., data = data_training) %>%
step_date(dt_customer, features = c("year", "month", "doy"), keep_original_cols = FALSE) %>%
step_date(date_col, features = c("year", "month", "doy"), keep_original_cols = FALSE) %>%
# handle unseen levels during cross-validation
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
# Fix zero variance before scaling and normalization
step_zv(all_predictors()) %>%
step_YeoJohnson(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
# Upsample minority class using SMOTE
step_smote(response)
# Prep and juice (or bake) the recipe on training data:
# Commenting this out because workflows take untrained data. It performs the
# prep and juice for use in a consistent manner.
# recipe_1 <- prep(recipe_1, training = data_training)
# training_preprocessed_1 <- juice(recipe_1)
# Recipe 2: Using downsampling for the majority class
recipe_2 <- recipe(response ~ ., data = data_training) %>%
step_date(dt_customer, features = c("year", "month", "doy"), keep_original_cols = FALSE) %>%
step_date(date_col, features = c("year", "month", "doy"), keep_original_cols = FALSE) %>%
# handle unseen levels during cross-validation
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
# Fix zero variance before scaling and normalization
step_zv(all_predictors()) %>%
step_YeoJohnson(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
# Downsample the majority class
step_downsample(response)
# recipe_2 <- prep(recipe_2, training = data_training)
# training_preprocessed_2 <- juice(recipe_2)
### 4. Model Algorithm Specifications
model_spec_1 <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
# Exclude hyper parameter tuning until later to use fit_resamples()
model_spec_2 <- rand_forest(mtry = 5, trees = 1000, min_n = 10) %>%
set_engine("ranger") %>%
set_mode("classification")
### 5. Workflow Objects
r1_m1_wkfl <- workflow() %>%
add_model(model_spec_1) %>%
add_recipe(recipe_1)
r1_m2_wkfl <- workflow() %>%
add_model(model_spec_2) %>%
add_recipe(recipe_1)
r2_m1_wkfl <- workflow() %>%
add_model(model_spec_1) %>%
add_recipe(recipe_2)
r2_m2_wkfl <- workflow()  %>%
add_model(model_spec_2) %>%
add_recipe(recipe_2)
### 6. Initial Cross-Validated Results
glimpse(marketing)
# Define custom metrics
# ROC AUC: Our model predicts who will respond to marketing offer. This metric shows how well our model predicts responders from non-responders.
# Accuracy: This metric shows how often our model gets it right.
# Precision: This metric shows out of those predicted to respond, how many actually did. This helps to recognize and avoid target non-responders and save money.
# Recall: This metric shows out of those who actually responded, how many did the model predict. This recognizes missing responders which loses revenue.
# F-1 score: This metric gives insight into balancing precision and recall.
custom_metrics <- metric_set(roc_auc, accuracy, precision, recall, f_meas)
# Creating cross validation folds
data_folds <- vfold_cv(data_training, v = 10, strata = response)
# Fit all four workflows with cross validation
r1_m1_fit <- r1_m1_wkfl %>%
fit_resamples(
resamples = data_folds,
metrics = custom_metrics,
control = control_resamples(save_pred = TRUE)
)
r1_m2_fit <- r1_m2_wkfl %>%
fit_resamples(
resamples = data_folds,
metrics = custom_metrics,
control = control_grid(save_pred = TRUE)
)
r2_m1_fit <- r2_m1_wkfl %>%
fit_resamples(
resamples = data_folds,
metrics = custom_metrics,
control = control_resamples(save_pred = TRUE)
)
r2_m2_fit <- r2_m2_wkfl %>%
fit_resamples(
resamples = data_folds,
metrics = custom_metrics,
control = control_grid(save_pred = TRUE)
)
# Combine all workflow results
workflow_results <- list(
r1_m1_fit = r1_m1_fit,
r1_m2_fit = r1_m2_fit,
r2_m1_fit = r2_m1_fit,
r2_m2_fit = r2_m2_fit
)
# Collect metrics for each workflow
workflow_perf_summary <- map_df(workflow_results, collect_metrics, .id = "workflow_name") %>%
mutate(
algorithm_name = case_when(
str_detect(workflow_name, "m1") ~ "Logistic Regression",
str_detect(workflow_name, "m2") ~ "Random Forest"
),
engine_name = case_when(
str_detect(workflow_name, "m1") ~ "glm",
str_detect(workflow_name, "m2") ~ "ranger"
)
)
print(workflow_perf_summary)
### 7. Promising Algorithm (Workflow) Selection
# Collect metrics from each fit object
r1_m1_metrics <- collect_metrics(r1_m1_fit) %>%
mutate(workflow_name = "r1_m1_wkfl", algorithm_name = "Logistic Regression", engine_name = "glm")
r1_m2_metrics <- collect_metrics(r1_m2_fit) %>%
mutate(workflow_name = "r1_m2_wkfl", algorithm_name = "Random Forest", engine_name = "ranger")
r2_m1_metrics <- collect_metrics(r2_m1_fit) %>%
mutate(workflow_name = "r2_m1_wkfl", algorithm_name = "Logistic Regression", engine_name = "glm")
r2_m2_metrics <- collect_metrics(r2_m2_fit) %>%
mutate(workflow_name = "r2_m2_wkfl", algorithm_name = "Random Forest", engine_name = "ranger")
# Combine all metrics into one tibble
workflow_perf_summary <- bind_rows(
r1_m1_metrics,
r1_m2_metrics,
r2_m1_metrics,
r2_m2_metrics
)
# Show the combined metrics table
print(workflow_perf_summary)
# View the top models based on ROC AUC (ability to distinguish responders and non-responders)
workflow_perf_summary %>%
filter(.metric == "roc_auc") %>%
arrange(desc(mean))
# View the top models based on F1 Score (cross check with precision and recall)
workflow_perf_summary %>%
filter(.metric == "f_meas") %>%
arrange(desc(mean))
# Since our business goal is to maximize responses while minimize false positives, we are prioritizing this workflow with the
# highest F1 score. This will help the model accurately predict responders without overestimating and wasting resources.
# Additionally, this workflow scored high on the ROC_AUC.
best_initial_workflow_fit <- r1_m2_fit
### 8. Hyperparameter Tuning
# Define a tuning grid
mrkt_rf_tune_spec <- rand_forest(
mtry = tune(),
min_n = tune(),
trees = tune()
) %>%
set_engine('ranger') %>%
set_mode('classification')
tunable_wkfl <- workflow() %>%
add_model(mrkt_rf_tune_spec) %>%
add_recipe(recipe_1)
# Extract parameters, create a grid to search, then search:
params <- extract_parameter_set_dials(tunable_wkfl) %>%
finalize(data_training)
set.seed(42)
tuning_grid <- grid_random(params, size = 15)
# Use tuning workflow to test combinations
set.seed(42)
tuning_results <- tunable_wkfl %>%
tune_grid(resamples = data_folds,
grid = tuning_grid,
metrics = custom_metrics)
# Review tuning results
tuning_results %>% collect_metrics()
best_rf_params <- select_best(tuning_results, metric = "f_meas")
select_best(tuning_results, metric = "f_meas")
tuning_results %>% collect_metrics() %>%
filter(.metric == "f_meas") %>%
arrange(desc(mean))
tuning_results %>% collect_metrics() %>%
filter(.metric == c("f_meas", "roc_auc")) %>%
arrange(desc(mean))
tuning_results %>%
collect_metrics() %>%
filter(.metric %in% c("f_meas", "roc_auc")) %>%
arrange(desc(mean))
select_best(tuning_results, metric = "f_meas")
# Finalize model with best hyperparameters
final_model <- finalize_model(model_spec_2, best_rf_params)
### 8. Hyperparameter Tuning
# Define a tuning grid
mrkt_rf_tune_spec <- rand_forest(
mtry = tune(),
min_n = tune(),
trees = tune()
) %>%
set_engine('ranger') %>%
set_mode('classification')
tunable_wkfl <- workflow() %>%
add_model(mrkt_rf_tune_spec) %>%
add_recipe(recipe_1)
# Extract parameters, create a grid to search, then search:
params <- extract_parameter_set_dials(tunable_wkfl) %>%
finalize(data_training)
set.seed(42)
tuning_grid <- grid_random(params, size = 15)
# Use tuning workflow to test combinations
registerDoParallel()
library(doParallel)
### 8. Hyperparameter Tuning
# Define a tuning grid
mrkt_rf_tune_spec <- rand_forest(
mtry = tune(),
min_n = tune(),
trees = tune()
) %>%
set_engine('ranger') %>%
set_mode('classification')
tunable_wkfl <- workflow() %>%
add_model(mrkt_rf_tune_spec) %>%
add_recipe(recipe_1)
# Extract parameters, create a grid to search, then search:
params <- extract_parameter_set_dials(tunable_wkfl) %>%
finalize(data_training)
set.seed(42)
tuning_grid <- grid_random(params, size = 15)
# Use tuning workflow to test combinations
registerDoParallel()
set.seed(42)
tuning_results <- tunable_wkfl %>%
tune_grid(resamples = data_folds,
grid = tuning_grid,
metrics = custom_metrics)
stopCluster()
